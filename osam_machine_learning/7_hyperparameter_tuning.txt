hyperparameter tuning

하이퍼 파라미터 : 모델을 학습하는데 구성하는 기본적인 변수들

1.Learning Rate

Supervised learning에서
WX + b = y   <==============> y 
                   loss(cost)
여기서 예측값과 실제값의 차이인 cost을 줄이는 과정이 머신러닝 과정이고

여기서 학습하는 과정에 cost를 줄이는 속도를 조절할 수 있는데 이것이 learning rate이다
learning rate 가 크다는 것은 미분의 방향(cost가 줄어드는 방향으로)으로 많이 이동

learning rate가 너무 작으면 굉장히 오랜동안 학습을 시켜야하는 단점
반대로 너무 크면 cost가 최소가 되는 곳에 가는것이 아닌 왔다 갔다 하면서 발산해버릴 수 있다. 

-Learning Rate Scheduling
 학습 속도에 따라서 rate를 변하게 하는 것
 -케라스에서는 learning rate scheduling을 위해서 굉장히 좋은 콜백함수를 가지고 있다
  (콜백함수는 예전에 early stopping에서 사용했었음->validation accuracy가 더이상 증가하지 않거나 validation loss가 줄어들지 않을때 학습을 멈추게 하는것 )
 def scheduler(epoch):
     if epoch < 10:
         return 0.001
     else:
         return 0.001 * tf.math.exp(0.1*(10 - epoch))
 
 callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
 model.fit(data, labels, epochs=100, callbacks=[callback], validation_data =(val_data, val_labels))
 
 
 -Epoch
  전체 데이터 셋에 대해 한 번의 학습 과정이 완료 
  -신경망에서 사용되는 역전파 알고리즘 : 파라미터를 사용하여 입력부터 출력까지의 각 계층의 weight를 계산하는 과정을 순방향 패스(forward pass), 반대로 거슬러 올라가면서 다시 한번 계산과정을 거처 기존의 weight를 수정하는 역방향 패스(backword pass)
  -이 전체 데이터 셋에 해당하는 과정(forword+backword)이 완료되면 한번의 epoch
  
  모델을 만들때 적절한 epoch 값을 설정해야만 under, overfiting을 방지
  epoch이 너무 작으면 underfiting, 너무 크면 overfiting이 발생할 확률이 높음
  
  
 -Batch Size
  한번에 학습시키는 수
  전체 데이터를 쪼개서 학습시킴
  ex)
   model.fit은 기본 batch size를 argument로 받지 않았었는데 default값으로 32로 한다
   -전체 데이터를 32등분으로 나눈다는 것
  
  batch_size를 너무 크게하면 예측값과 실제값의 차이를 이용한 MSE(최소제곱법-제곱 평균이 최소가 되게 한다.)에서 평균값이 너무 작아져서 최소값을 못찾는 경우가 발생-learning rate를 너무 작게 한것과 비슷한 건가?
  반대도 마찬가지
   
 -Iteration
  epoch을 나누어서 실행하는 횟수
  
 메모리의 한계와 속도 저하 때문에 대부분의 경우 한번의 epoch에서 모든 데이터를 한꺼번에 집어넣을 수는 없다.
 그래서 데이터를 나누어서 주게 되는데 이때 몇 번 나누어서 주는가를 iteration, 각 iteration마다 주는 데이터 사이즈를 batch size라고 한다. 
 
(example)
전체 2,000개의 데이터, epochs =20, batch_size = 500
=> 1 epoch은 4번의 iteration
    전체 20 번의 학습이 이루어졌고, iteration 기준으로는 80번의 학습이 이루어짐


 -Finding Optimal Hyperparameters
  하이퍼파라미터의 최적값을 찾는 방법
  -Gird Search
  각각의 변수를 이용한 테이블을 만들어서 하나하나 확인(실험)하면서 최적의 값을 찾는 방법
  -단점 : gird search에서는 테이블을 만들때 변수들의 값들을 임의로 정해줘야 하는데 정작 최솟값은 놓치는 경우가 발생할 수 있음(연속적이지 않기 때문에)
  
  -Random Search
   일정한 간격으로 변수 테이블을 만드는것이 아닌 random으로 찾고 나온 최적값 주의에서 다시 찾고 이런방식으로 찾음
   
  -AutoML
   최적의 파라미터 값을 자동으로 찾아줌
   -katib, AutoKeras등이 있음
  
  
  
  
 -Overfiting 피하기
 high variance, low bias 라고도 불림
 
 4가지 방법
 -Collect more data
     데이터를 많이 모아서 특정적으로 경향화되는 것을 막는다
     
 -Early stopping
     overfiting이 되기전에 일정 수준 accuracy or loss가 되면 멈춘다. 
     early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10)
     model.fit(train_data,train_labels, epochs=EPOCHS, validation_split = 0.2, callbacks = [early_stop])
     
     
 -Regularization
     일부 parameter를 작은 값으로 만들어 함수가 심플해지도록 하는 것
     cost에 추가적인 loss를 더하는것
     L1, L2 regulation존재
     L1의 경우 cost += 람다*sum|Wj|
     L2의 경우 cost += 람다*sum W^2
     의미는 weight가 행렬값에 대해 너무 커지지 않기를 원하는 것
     너무 크거나 불필요한것은 0으로 만들어 버리는 것(cost는 최소값을 찾기 때문에 w가 커지는 놈이 0으로 가버린다.)
     tf.keras.layers.Dens(3,kernel_regularizer ='l1')
     
 -Dropouts
     학습시킬때 에폭이 끝날 때마다 랜덤으로 몇개의 뉴런을 없앤다.=>심플한 모델이 된다.
     테스트 할때는 없어지는거 없이 모든 뉴런으로 한다.
     =>마치 여러가지 심플한 모델의 조합(앙상블)으로 표현이 됨
     h = layers.Dense(64, activation = 'relu')(inputs)
     h = layers.Dense(32)(h)
                     |
                     V
    h = layers.Dense(64, activation = 'relu')(inputs)
    h = layers.Dropout(0.3)(h)
    h = layers.Dense(32)(h)
     
     
     
     
     
 -Weight initialization
     weight = trainable parameters
     WX + b = y
     W,b는  weight
     
     keras에서는 tf.keras.layers.Dense할때 초기값을 뭘로 줄까?
     kernel_initializer = "glorot_uniform"으로 주고 있음
     glorot Unirom (Xavier Uniform)에서는 
     [-limit,limit]사이에서 값을 랜덤으로 뽑는다
     여기서 limit = sqrt(6/(in+out)) in, out dimension
     ex)input shape이 (28,28)이고 ouput이 128이라면
     in = 784, out = 128
     
     
     -He Initialization
         W ~ N(0,Var(W))
         -weight를 정규분포에서 선택한다는 것
         -var(W) = root(2/input demension)
         -relu를 activation function으로 사용할 때 효과적임
                 
     
 
 


 
   
   
 