bagging과 boosting

앙상블 학습(기법)
- 여러 개의 결정트리를 결합하여 하나의 결정 트리보다 더 좋은 성능을 내는 머신러닝 기법
- 핵심은 여러개의 약 분류기(weak classifier)를 결합하여 강 분류기(strong classifier)를 만드는것

- bagging과 boosting으로 이루어짐


bagging
- 샘플을 여러번 뽑아 각 모델을 학습시켜 결과를 집계하는 방법
- Bootstrap Aggregating

boosting
- bagging과 동일하게 복원랜덤 샘플링을 하지만 병렬적으로 모델을 만들어 결과를 추합하는것이 아닌 
  한 모델의 학슴이 끝나면 결과에 따라 가중치를 재분배하여 다시 학습을 반복
- 전에 했던 학습이 다음 학습에 영향을 줌
- adaboost, xgboost등이 있음

각각의 장단점

                   Bagging                 |           Boosting
                   Random Forest           |           AdaBoost
______________________________________________________________________________________
장점    |                                   |
        | -Overfitting의 위험이 적음         |  
        | -병렬처리로 빠른 실행 가능           |      -일반적으로 accuracy가 높음
        |                                  |
        |                                  |                                            
--------------------------------------------------------------------------------------
단점   |                                    |
        |    -Boosting에 비해 accuracy가     |        -Overfitting의 위험이 있음
        |     낮을 수 있음                   |        -순차적으로 실행되어 시간이 오래걸림
        |                                   |
        |                                   |
