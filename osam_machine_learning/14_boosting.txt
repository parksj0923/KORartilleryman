Boosting도 마찬가지로 앙상블 기법중 하나


먼저 Bootstrap은 샘플을 여러번 뽑는것을 의미(복원 랜덤 샘플링))

boosting은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법

배깅의 경우 Decision Tree 1,2가 서로 독립적으로 결과를 예측, 그 결과를 집계해 최종 결과값을 예측하는 방식

부스팅의 경우 모델간 연관성이 있음. 처음 모델이 예측하면 그 예측결과에 따라 데이터에 가중치가 부여되고, 부여된 가중치가 다음 모델에 영향을 준다. 

학습을 통해 분류했을때 잘못 분류된 데이터에 집중하여 새로운 규칙을 만들어 다시 결과를 내는 것을 반복

각 모델을 만들때 dataset은 어떻게 만들까?

각 모델 결과의 weight는 어떻게 구할까?

대표적인 기법마다 다르긴 한데 AdaBoost를 통해 알아보자

AdaBoost 
-첫 번째 classifier를 학습시키기 위해 전체 training set 중 random sampling
-에러(i.e.MSE, MAE, Gini)로부터 model의 weight를 계산
-에측이 틀린 training data point의 weight를 갱신 --> 다음 bootstrap에서 더 잘 뽑히도록
(틀린 데이터에 가중치를 더 주는 것임)

실습 코드는 코랩에 있음 